{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afff2eca",
   "metadata": {},
   "source": [
    "# Converting .abf to NWB\n",
    "- see `clampfit_abf_standardization_nwb.pptx`\n",
    "- see Sharepoint: `Documents/data/data_standardization/pclamp_clampfit_icephys`\n",
    "- see `ephys_nwb_params.xlsx`: this is maintained by experimenter\n",
    "- **BE SURE TO SET**\n",
    "    - **EXCEL FILE NAME** (`ephys_nwb_params.xlsx` in this example)\n",
    "    - **ABF DIRECTORY** (`abf_data` in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa57d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pacody/Documents/code/icephys_abf2nwb/env/lib/python3.13/site-packages/pynwb/file.py:158: UserWarning: Date is missing timezone information. Updating to local timezone.\n",
      "  args_to_set['date_of_birth'] = _add_missing_timezone(date_of_birth)\n",
      "/Users/pacody/Documents/code/icephys_abf2nwb/env/lib/python3.13/site-packages/pynwb/file.py:486: UserWarning: Date is missing timezone information. Updating to local timezone.\n",
      "  args_to_set['session_start_time'] = _add_missing_timezone(session_start_time)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Finished NWB conversion for experiment 2025053001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pacody/Documents/code/icephys_abf2nwb/env/lib/python3.13/site-packages/pynwb/file.py:158: UserWarning: Date is missing timezone information. Updating to local timezone.\n",
      "  args_to_set['date_of_birth'] = _add_missing_timezone(date_of_birth)\n",
      "/Users/pacody/Documents/code/icephys_abf2nwb/env/lib/python3.13/site-packages/pynwb/file.py:486: UserWarning: Date is missing timezone information. Updating to local timezone.\n",
      "  args_to_set['session_start_time'] = _add_missing_timezone(session_start_time)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Finished NWB conversion for experiment 2025053101\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from neuroconv.datainterfaces import AbfInterface\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to Excel and ABF files\n",
    "excel_path = Path(\"./ephys_nwb_params.xlsx\")\n",
    "ECEPHY_DATA_PATH = Path(\"./abf_data\")\n",
    "output_folder = Path(\"./nwb_files\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "log_file = output_folder / f\"conversion_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "error_csv = output_folder / f\"error_experiments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "def log_message(message, print_to_console=True):\n",
    "    \"\"\"Write message to log file and optionally print to console\"\"\"\n",
    "    with open(log_file, 'a') as f:\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        f.write(f\"[{timestamp}] {message}\\n\")\n",
    "    if print_to_console:\n",
    "        print(message)\n",
    "\n",
    "# Track errors for CSV output\n",
    "error_records = []\n",
    "\n",
    "# Load Excel file\n",
    "log_message(\"Starting NWB conversion process\")\n",
    "log_message(f\"Loading Excel file: {excel_path}\")\n",
    "\n",
    "df = pd.read_excel(excel_path)\n",
    "df = df.iloc[1:]  # Drop the first row after the header\n",
    "\n",
    "# Clean data: remove completely empty rows and rows with missing critical data\n",
    "initial_row_count = len(df)\n",
    "df = df.dropna(how=\"all\")  # Remove completely empty rows\n",
    "df = df.dropna(subset=[\"EXPERIMENT ID\", \".abf file\"], how=\"any\")  # Must have these\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "rows_removed = initial_row_count - len(df)\n",
    "log_message(f\"Removed {rows_removed} empty or incomplete rows. {len(df)} rows remaining.\")\n",
    "\n",
    "# Helper function to safely get values (returns empty string for NaN)\n",
    "def safe_get(value, default=\"\"):\n",
    "    return value if pd.notna(value) else default\n",
    "\n",
    "# Helper function to ensure .abf extension\n",
    "def ensure_abf_extension(filename):\n",
    "    \"\"\"Append .abf extension if not already present\"\"\"\n",
    "    if pd.isna(filename):\n",
    "        return filename\n",
    "    filename = str(filename).strip()\n",
    "    if not filename.lower().endswith('.abf'):\n",
    "        filename += '.abf'\n",
    "    return filename\n",
    "\n",
    "# Apply .abf extension fix to the column\n",
    "df[\".abf file\"] = df[\".abf file\"].apply(ensure_abf_extension)\n",
    "\n",
    "# Group by EXPERIMENT ID\n",
    "grouped = df.groupby(\"EXPERIMENT ID\")\n",
    "total_experiments = len(grouped)\n",
    "log_message(f\"Processing {total_experiments} experiments\")\n",
    "\n",
    "successful_conversions = 0\n",
    "failed_conversions = 0\n",
    "\n",
    "for experiment_id, group in grouped:\n",
    "    try:\n",
    "        log_message(f\"\\n--- Processing experiment {experiment_id} ---\")\n",
    "        first_row = group.iloc[0]\n",
    "\n",
    "        # Construct icephys_metadata with safe value extraction\n",
    "        icephys_metadata = {\n",
    "            \"cell_id\": safe_get(first_row[\"cell_id\"]),\n",
    "            \"slice_id\": safe_get(first_row[\"slice_id\"]),\n",
    "            \"targeted_layer\": safe_get(first_row[\"targeted_layer\"]),\n",
    "            \"inferred_layer\": safe_get(first_row.get(\"inferred_layer\", \"\")),\n",
    "            \"recording_sessions\": [\n",
    "                {\n",
    "                    \"abf_file_name\": safe_get(row[\".abf file\"]),\n",
    "                    \"stimulus_type\": safe_get(row[\"stimulus_type\"]),\n",
    "                    \"icephys_experiment_type\": safe_get(row[\"icephys_experiment_type\"])\n",
    "                }\n",
    "                for _, row in group.iterrows()\n",
    "                if pd.notna(row[\".abf file\"])  # Extra safety check\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Verify ABF files exist\n",
    "        abf_file_paths = []\n",
    "        missing_files = []\n",
    "        for _, row in group.iterrows():\n",
    "            if pd.notna(row[\".abf file\"]):\n",
    "                file_path = ECEPHY_DATA_PATH / row[\".abf file\"]\n",
    "                if not file_path.exists():\n",
    "                    warning_msg = f\"Warning: ABF file not found: {file_path}\"\n",
    "                    log_message(warning_msg)\n",
    "                    missing_files.append(row[\".abf file\"])\n",
    "                    continue\n",
    "                abf_file_paths.append(file_path)\n",
    "\n",
    "        if not abf_file_paths:\n",
    "            error_msg = f\"Skipping experiment {experiment_id}: no valid ABF files found\"\n",
    "            log_message(error_msg)\n",
    "            # Add all ABF files from this experiment to error records\n",
    "            for _, row in group.iterrows():\n",
    "                error_records.append({\n",
    "                    \"EXPERIMENT ID\": experiment_id,\n",
    "                    \".abf file\": row[\".abf file\"],\n",
    "                    \"error_type\": \"no_valid_files\"\n",
    "                })\n",
    "            failed_conversions += 1\n",
    "            continue\n",
    "\n",
    "        log_message(f\"Found {len(abf_file_paths)} valid ABF files for experiment {experiment_id}\")\n",
    "\n",
    "        # Instantiate data interface\n",
    "        interface = AbfInterface(\n",
    "            file_paths=abf_file_paths,\n",
    "            icephys_metadata=icephys_metadata\n",
    "        )\n",
    "\n",
    "        # Retrieve and update metadata\n",
    "        metadata = interface.get_metadata()\n",
    "        metadata['NWBFile'].update(\n",
    "            identifier=str(experiment_id),\n",
    "            session_description=safe_get(first_row[\"session_description\"]),\n",
    "            lab=\"my lab name\",\n",
    "            institution=\"My University\",\n",
    "            experimenter=[\"John Doe\", \"Jane Doe\"]\n",
    "        )\n",
    "        metadata[\"Subject\"] = {\n",
    "            \"subject_id\": safe_get(first_row[\"subject_id\"]),\n",
    "            \"species\": safe_get(first_row[\"species\"]),\n",
    "            \"genotype\": safe_get(first_row[\"genotype\"]),\n",
    "            \"sex\": safe_get(first_row[\"sex\"]),\n",
    "            \"date_of_birth\": str(safe_get(first_row[\"date_of_birth\"]))\n",
    "        }\n",
    "\n",
    "        # Run conversion\n",
    "        nwb_output_path = output_folder / f\"{experiment_id}.nwb\"\n",
    "        interface.run_conversion(nwbfile_path=nwb_output_path, metadata=metadata)\n",
    "\n",
    "        success_msg = f\"✓ Finished NWB conversion for experiment {experiment_id}\"\n",
    "        log_message(success_msg)\n",
    "        successful_conversions += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"✗ Error processing experiment {experiment_id}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        # Add all ABF files from this experiment to error records\n",
    "        for _, row in group.iterrows():\n",
    "            error_records.append({\n",
    "                \"EXPERIMENT ID\": experiment_id,\n",
    "                \".abf file\": row[\".abf file\"],\n",
    "                \"error_type\": f\"conversion_error: {str(e)}\"\n",
    "            })\n",
    "        failed_conversions += 1\n",
    "        continue\n",
    "\n",
    "# Write summary to log\n",
    "log_message(\"\\n\" + \"=\"*50)\n",
    "log_message(\"CONVERSION SUMMARY\")\n",
    "log_message(\"=\"*50)\n",
    "log_message(f\"Total experiments: {total_experiments}\")\n",
    "log_message(f\"Successful conversions: {successful_conversions}\")\n",
    "log_message(f\"Failed conversions: {failed_conversions}\")\n",
    "log_message(f\"Success rate: {(successful_conversions/total_experiments)*100:.1f}%\")\n",
    "\n",
    "# Save error CSV\n",
    "if error_records:\n",
    "    error_df = pd.DataFrame(error_records)\n",
    "    error_df.to_csv(error_csv, index=False)\n",
    "    log_message(f\"\\nError details saved to: {error_csv}\")\n",
    "    log_message(f\"Total error records: {len(error_records)}\")\n",
    "else:\n",
    "    log_message(\"\\nNo errors encountered - all conversions successful!\")\n",
    "\n",
    "log_message(f\"\\nLog file saved to: {log_file}\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Conversion complete! Check {log_file} for details.\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
